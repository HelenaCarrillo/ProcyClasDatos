{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW_jk7BvHgs7",
        "outputId": "c7575909-d9f0-4bc8-ea65-d76bce3af4ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hub in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: deeplake in /usr/local/lib/python3.10/dist-packages (from hub) (3.9.12)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (1.25.2)\n",
            "Requirement already satisfied: pillow~=10.2.0 in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (10.2.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (1.34.131)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (8.1.7)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (0.3.2)\n",
            "Requirement already satisfied: humbug>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (0.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (4.66.4)\n",
            "Requirement already satisfied: lz4 in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (4.3.3)\n",
            "Requirement already satisfied: pyjwt in /usr/lib/python3/dist-packages (from deeplake->hub) (2.3.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (2.8.0)\n",
            "Requirement already satisfied: libdeeplake==0.0.134 in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (0.0.134)\n",
            "Requirement already satisfied: aioboto3>=10.4.0 in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (13.1.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from deeplake->hub) (1.6.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from libdeeplake==0.0.134->deeplake->hub) (0.3.8)\n",
            "Requirement already satisfied: aiobotocore[boto3]==2.13.1 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake->hub) (2.13.1)\n",
            "Requirement already satisfied: aiofiles>=23.2.1 in /usr/local/lib/python3.10/dist-packages (from aioboto3>=10.4.0->deeplake->hub) (24.1.0)\n",
            "Requirement already satisfied: botocore<1.34.132,>=1.34.70 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (1.34.131)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (3.9.5)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (1.14.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (0.11.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake->hub) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3->deeplake->hub) (0.10.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from humbug>=0.3.1->deeplake->hub) (2.31.0)\n",
            "Requirement already satisfied: ppft>=1.7.6.8 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake->hub) (1.7.6.8)\n",
            "Requirement already satisfied: pox>=0.3.4 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake->hub) (0.3.4)\n",
            "Requirement already satisfied: multiprocess>=0.70.16 in /usr/local/lib/python3.10/dist-packages (from pathos->deeplake->hub) (0.70.16)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deeplake->hub) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deeplake->hub) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->deeplake->hub) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.132,>=1.34.70->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.34.132,>=1.34.70->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (2.0.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake->hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake->hub) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->humbug>=0.3.1->deeplake->hub) (2024.6.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.9.2->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.132,>=1.34.70->aiobotocore[boto3]==2.13.1->aioboto3>=10.4.0->deeplake->hub) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3umIftdIBL_",
        "outputId": "924821ed-b55d-4e76-9306-cc9626931786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (12.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import hub\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "import pywt\n",
        "import pickle\n",
        "\n",
        "import random\n",
        "from random import seed, random, randint, sample\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "gzxXmgj_IEEp"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "from keras import backend as K\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Input, Dense, GlobalMaxPool1D, Activation, MaxPool1D, Conv1D, Flatten, BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "#from keras.utils.vis_utils import plot_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "UD2-3Dgkkop-"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = hub.load(\"hub://activeloop/spoken_mnist\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKhWVH2sJ44B",
        "outputId": "ce021250-bab3-49e7-f7c9-77a9862340f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\\"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/spoken_mnist\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\\"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://activeloop/spoken_mnist loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r\r\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio = []\n",
        "yy = []\n",
        "for i in tqdm(range(3000)) :\n",
        "  if ds[i]['speakers'].data()['value'] == 'george':\n",
        "    audio.append(ds[i]['audio'].numpy()[:,0])\n",
        "    yy.append(0)\n",
        "  if ds[i]['speakers'].data()['value'] == 'jackson':\n",
        "    audio.append(ds[i]['audio'].numpy()[:,0])\n",
        "    yy.append(1)\n",
        "  if ds[i]['speakers'].data()['value'] == 'lucas':\n",
        "    audio.append(ds[i]['audio'].numpy()[:,0])\n",
        "    yy.append(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs41JrTnN1G9",
        "outputId": "81a953d4-bec0-4501-c77b-08f218f28430"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [00:18<00:00, 164.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "audio_train, audio_test, y_train, y_test = train_test_split(audio, yy, test_size=0.3)"
      ],
      "metadata": {
        "id": "7Hs6iSaoPwJt"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Data class distribution: \", np.unique(y_train, return_counts=True))\n",
        "print(\"Testing Data class distribution: \", np.unique(y_test, return_counts=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmbiQStnSVvi",
        "outputId": "12599817-e9a4-4413-b1be-7bf811cb4d1e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data class distribution:  (array([0, 1, 2]), array([342, 348, 360]))\n",
            "Testing Data class distribution:  (array([0, 1, 2]), array([158, 152, 140]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_wavelet_features(X) :\n",
        "\n",
        "    # Define a few parameters\n",
        "    wavelet = 'morl' # wavelet type: morlet\n",
        "    sr = 8000 # sampling frequency: 8KHz\n",
        "    widths = np.arange(1, 256) # scales for morlet wavelet\n",
        "    dt = 1/sr # timestep difference\n",
        "\n",
        "    frequencies = pywt.scale2frequency(wavelet, widths) / dt # Get frequencies corresponding to scales\n",
        "\n",
        "    # Create a filter to select frequencies between 80Hz and 5KHz\n",
        "    upper = ([x for x in range(len(widths)) if frequencies[x] > 1000])[-1]\n",
        "    lower = ([x for x in range(len(widths)) if frequencies[x] < 80])[0]\n",
        "    widths = widths[upper:lower] # Select scales in this frequency range\n",
        "\n",
        "    # Compute continuous wavelet transform of the audio numpy array\n",
        "    wavelet_coeffs, freqs = pywt.cwt(X, widths, wavelet = wavelet, sampling_period=dt)\n",
        "    # print(wavelet_coeffs.shape)\n",
        "    # sys.exit(1)\n",
        "\n",
        "    # Split the coefficients into frames of length 800\n",
        "    start = 0\n",
        "    end = wavelet_coeffs.shape[1]\n",
        "    frames = []\n",
        "    frame_size = 400\n",
        "    count = 0\n",
        "\n",
        "    while start+frame_size <= end-1 :\n",
        "\n",
        "        f = wavelet_coeffs[:,start:start+frame_size]\n",
        "\n",
        "        # Total samples in a frame will not be a multiple of 800 everytime. If the last frame length is less than 800, we can skip it.\n",
        "        assert f.shape[1] == frame_size # assert frame lengths are equal to the frame_size parameter\n",
        "\n",
        "        frames.append(f)\n",
        "        start += frame_size\n",
        "\n",
        "\n",
        "    # Convert frames to numpy array\n",
        "    frames = np.array(frames)\n",
        "    frames = frames.reshape((len(frames), wavelet_coeffs.shape[0], frame_size))\n",
        "\n",
        "    return frames"
      ],
      "metadata": {
        "id": "JfUQdy7uS3Ej"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Compute Training data features. We have each sample into frames of length 400\n",
        "\n",
        "indices = []\n",
        "WaveletFeatTrain = [] # Store wavelet features\n",
        "WaveletYTrain = [] # Store class labels corresponding to wavelet features from an audio sample\n",
        "uniq_id = []\n",
        "count = 0\n",
        "\n",
        "for i in range(3) :\n",
        "\n",
        "    array = np.array(y_train)\n",
        "    ind = np.where(array == i)\n",
        "    seed(i)\n",
        "    ind = ind[0].tolist()\n",
        "    ind = sample(ind, 100)\n",
        "    audio_samples = []\n",
        "\n",
        "    for l in range(len(ind)):\n",
        "      audio_samples.append(audio_train[l])\n",
        "\n",
        "    num_rand_samp = 100\n",
        "\n",
        "    for j in tqdm(range(len(audio_samples))) :\n",
        "\n",
        "        # print(\"i \", i, \" j \", j, \"/\", len(audio_samples))\n",
        "        curr_sample = audio_samples[j]\n",
        "        #seq, _ = librosa.load(curr_sample)\n",
        "        F = compute_wavelet_features(curr_sample)\n",
        "        F = F.astype(np.float16)\n",
        "\n",
        "        # Generate target labels corresponding to the frames of each sample\n",
        "        indices = np.arange(0, len(F), 1)\n",
        "        indices = indices.tolist()\n",
        "        indices = sample(indices, min(num_rand_samp, len(indices)))\n",
        "        F = F[indices]\n",
        "        uniq_id += [count] * len(F)\n",
        "        WaveletYTrain += [i] * len(F)\n",
        "\n",
        "        if count == 0 :\n",
        "            WaveletFeatTrain = F\n",
        "        else :\n",
        "            WaveletFeatTrain = np.concatenate((WaveletFeatTrain, F), axis=0)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "\n",
        "\n",
        "print(\"X: \", WaveletFeatTrain.shape)\n",
        "\n",
        "WaveletYTrain = np.array(WaveletYTrain) # Convert to numpy array\n",
        "uniq_id = np.array(uniq_id)\n",
        "print(\"Y: \", WaveletYTrain.shape, \" unique: \", np.unique(WaveletYTrain, return_counts=True))\n",
        "# Write all features to a .npz file\n",
        "np.savez_compressed(os.getcwd()+\"/training_features\", a=WaveletFeatTrain, b=WaveletYTrain, c=uniq_id)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENGSxUWYT6FR",
        "outputId": "2d18b076-22e6-47a1-f43b-a8beaf5a834d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 10/100 [00:00<00:05, 15.42it/s]<ipython-input-120-2306bf3fabde>:29: RuntimeWarning: overflow encountered in cast\n",
            "  F = F.astype(np.float16)\n",
            "100%|██████████| 100/100 [00:09<00:00, 11.07it/s]\n",
            "100%|██████████| 100/100 [00:12<00:00,  7.71it/s]\n",
            "100%|██████████| 100/100 [00:17<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  (2919, 76, 400)\n",
            "Y:  (2919,)  unique:  (array([0, 1, 2]), array([973, 973, 973]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Compute Testing data features\n",
        "\n",
        "WaveletFeatTest = [] # Store wavelet features. We have each sample into frames of length 400\n",
        "WaveletYTest = [] # Store class labels corresponding to wavelet features from an audio sample\n",
        "uniq_id = []\n",
        "\n",
        "for i in tqdm(range(len(audio_test))) :\n",
        "\n",
        "    curr_sample = audio_test[i]\n",
        "    #seq, _ = librosa.load(curr_sample)\n",
        "    curr_target = y_test[i]\n",
        "    F = compute_wavelet_features(curr_sample)\n",
        "\n",
        "    # Generate target labels corresponding to the frames of each sample\n",
        "    WaveletYTest += [curr_target] * len(F)\n",
        "    uniq_id += [i] * len(F)\n",
        "\n",
        "    if i == 0 :\n",
        "        WaveletFeatTest = F\n",
        "    else :\n",
        "        WaveletFeatTest = np.concatenate((WaveletFeatTest, F), axis=0)\n",
        "\n",
        "WaveletYTest = np.array(WaveletYTest) # Convert to numpy array\n",
        "uniq_id = np.array(uniq_id)\n",
        "print(\"X: \", WaveletFeatTest.shape, \"  y: \", WaveletYTest.shape)\n",
        "\n",
        "WaveletFeatTest = WaveletFeatTest.astype(np.float16)\n",
        "\n",
        "# Write all features to a .npz file\n",
        "np.savez_compressed(os.getcwd()+\"/testing_features\", a=WaveletFeatTest, b=WaveletYTest, c=uniq_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vi7e2Q6FiVzt",
        "outputId": "7f7ec671-3c03-4379-8f1c-11dce6d908e8"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 450/450 [03:12<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X:  (4268, 76, 400)   y:  (4268,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-121-c0380dde07c3>:27: RuntimeWarning: overflow encountered in cast\n",
            "  WaveletFeatTest = WaveletFeatTest.astype(np.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear modelo"
      ],
      "metadata": {
        "id": "wghduS7Zkf5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(row, col) :\n",
        "\n",
        "    n_filters = 32\n",
        "    filter_width = 3\n",
        "    dilation_rates = [2**i for i in range(6)] * 2\n",
        "\n",
        "    # define an input history series and pass it through a stack of dilated causal convolution blocks\n",
        "    history_seq = Input(shape=(row, col))\n",
        "    x = history_seq\n",
        "\n",
        "    skips = []\n",
        "    count = 0\n",
        "    # x = GaussianNoise(0.01)(x)\n",
        "    for dilation_rate in dilation_rates:\n",
        "\n",
        "        # preprocessing - equivalent to time-distributed dense\n",
        "\n",
        "        # filter\n",
        "        x = Conv1D(filters=n_filters,\n",
        "                    kernel_size=filter_width,\n",
        "                    padding='causal',\n",
        "                    dilation_rate=dilation_rate, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(x)\n",
        "\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    out = Conv1D(16, 3, padding='same', kernel_initializer= 'random_normal', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(x)\n",
        "    out = BatchNormalization()(out)\n",
        "    out = Activation('relu')(out)\n",
        "    out = GlobalMaxPool1D()(out)\n",
        "\n",
        "    out = Dense(3, kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001))(out)\n",
        "    out = Activation('softmax')(out)\n",
        "\n",
        "    model = Model(history_seq, out)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model(400, 76)\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl1S8Bw7kfVo",
        "outputId": "47c6745e-9522-4cff-a416-11a130ddf118"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 400, 76)]         0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 400, 32)           7328      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 400, 32)           128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 400, 32)           128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 400, 32)           128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 400, 16)           1552      \n",
            "                                                                 \n",
            " batch_normalization_12 (Ba  (None, 400, 16)           64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 400, 16)           0         \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 16)                0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 51        \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44675 (174.51 KB)\n",
            "Trainable params: 43875 (171.39 KB)\n",
            "Non-trainable params: 800 (3.12 KB)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train model"
      ],
      "metadata": {
        "id": "FHD_XOLEphgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "training_data = np.load(os.getcwd()+\"/training_features.npz\")\n",
        "X = training_data['a']\n",
        "y = training_data['b']\n",
        "\n",
        "X = X.transpose(0,2,1) # Put data in correct format: Num_samples x timesteps x features\n",
        "y = to_categorical(y) # Convert class labels to categorial vectors\n",
        "print(\"X \", X.shape, \"y \", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEG2M1QYms2T",
        "outputId": "8ffdb4ac-e31f-4747-8917-a62f97e9ba9c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X  (2919, 400, 76) y  (2919, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize the data\n",
        "mean = X.mean()\n",
        "std = X.std()\n",
        "X = (X-mean)/ std\n",
        "\n",
        "print(\"Mean \", mean, \" STD \", std, X.mean(), X.std())\n",
        "\n",
        "X = X.astype(np.float16)\n",
        "\n",
        "y = y.astype(np.uint8)\n",
        "\n",
        "print(\"Input shapes \", X.shape, y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQZHUPKgpbAP",
        "outputId": "addf1595-1f93-43c7-f7a2-1fa6ceac5b7b"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:118: RuntimeWarning: invalid value encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:152: RuntimeWarning: overflow encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:152: RuntimeWarning: invalid value encountered in reduce\n",
            "  arrmean = umr_sum(arr, axis, dtype, keepdims=True, where=where)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean  nan  STD  nan nan nan\n",
            "Input shapes  (2919, 400, 76) (2919, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the standard deviation and mean in a pickle file\n",
        "f = open(os.getcwd()+'/speaker_mean_std.pkl', 'wb')\n",
        "pickle.dump([mean, std, y], f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "oinrQU2Ppfv4"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### ANTES DEL MODELO\n",
        "\n",
        "r,c = X[0].shape\n",
        "\n",
        "# Split data into training and validation\n",
        "X1, Xval, y1, yval = train_test_split(X, y, test_size=0.20)#, random_state=int(time.time()))\n",
        "\n",
        "# Use 5-fold cross validation\n",
        "kfold = StratifiedKFold(n_splits=5, shuffle=True)"
      ],
      "metadata": {
        "id": "3tC9Hwk5qrNk"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "\n",
        "# Train the model\n",
        "for train, test in kfold.split(X1, np.argmax(y1, axis= -1)):\n",
        "\n",
        "    print(\"K Fold Step  \", count)\n",
        "    model.fit(X1[train], y1[train], validation_data= (X1[test], y1[test]), batch_size= 128, epochs= 80, verbose= 2)\n",
        "    model.save(os.getcwd()+\"/speaker_classifier.h5\")\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    scores = model.evaluate(Xval, yval, verbose=0)\n",
        "    print(\"Metrics : \", scores)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5wTWwRAp50d",
        "outputId": "10775628-5bc4-4537-c773-369dcb5bd7cd"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K Fold Step   0\n",
            "Epoch 1/80\n",
            "15/15 - 36s - loss: nan - val_loss: nan - 36s/epoch - 2s/step\n",
            "Epoch 2/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 3/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 4/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 5/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 6/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 7/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 8/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 9/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 10/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 11/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 12/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 13/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 14/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 15/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 16/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 17/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 18/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 19/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 20/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 21/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 22/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 23/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 24/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 25/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 26/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 27/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 28/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 29/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 30/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 31/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 32/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 33/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 34/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 35/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 36/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 37/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 38/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 39/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 40/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 41/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 42/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 43/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 44/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 45/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 46/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 47/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 48/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 49/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 50/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 51/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 52/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 53/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 54/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 55/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 56/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 57/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 58/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 59/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 60/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 61/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 62/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 63/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 64/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 65/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 66/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 67/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 68/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 69/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 70/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 71/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 72/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 73/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 74/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 75/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 76/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 77/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 78/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 79/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 80/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics :  nan\n",
            "K Fold Step   1\n",
            "Epoch 1/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 2/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 3/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 4/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 5/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 6/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 7/80\n",
            "15/15 - 17s - loss: nan - val_loss: nan - 17s/epoch - 1s/step\n",
            "Epoch 8/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 9/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 10/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 11/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 12/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 13/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 14/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 15/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 16/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 17/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 18/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 19/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 20/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 21/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 22/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 23/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 24/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 25/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 26/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 27/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 28/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 29/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 30/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 31/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 32/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 33/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 34/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 35/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 36/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 37/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 38/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 39/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 40/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 41/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 42/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 43/80\n",
            "15/15 - 17s - loss: nan - val_loss: nan - 17s/epoch - 1s/step\n",
            "Epoch 44/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 45/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 46/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 47/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 48/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 49/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 50/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 51/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 52/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 53/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 54/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 55/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 56/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 57/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 58/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 59/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 60/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 61/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 62/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 63/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 64/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 65/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 66/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 67/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 68/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 69/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 70/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 71/80\n",
            "15/15 - 17s - loss: nan - val_loss: nan - 17s/epoch - 1s/step\n",
            "Epoch 72/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 73/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 74/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 75/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 76/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 77/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 78/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 79/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 80/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Metrics :  nan\n",
            "K Fold Step   2\n",
            "Epoch 1/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 2/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 3/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 4/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 5/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 6/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 7/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 8/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 9/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 10/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 11/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 12/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 13/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 14/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 15/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 16/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 17/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 18/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 19/80\n",
            "15/15 - 17s - loss: nan - val_loss: nan - 17s/epoch - 1s/step\n",
            "Epoch 20/80\n",
            "15/15 - 17s - loss: nan - val_loss: nan - 17s/epoch - 1s/step\n",
            "Epoch 21/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 22/80\n",
            "15/15 - 17s - loss: nan - val_loss: nan - 17s/epoch - 1s/step\n",
            "Epoch 23/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 24/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 25/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 26/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 27/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 28/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 29/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 30/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 31/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 32/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 33/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 34/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 35/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 36/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 37/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 38/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 39/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 40/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 41/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 42/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 43/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 44/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 45/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 46/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 47/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 48/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 49/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 50/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 51/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 52/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 53/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 54/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 55/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 56/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 57/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 58/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 59/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 60/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 61/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 62/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 63/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 64/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 65/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 66/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 67/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 68/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 69/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 70/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 71/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 72/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 73/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 74/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 75/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 76/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 77/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 78/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 79/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 80/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Metrics :  nan\n",
            "K Fold Step   3\n",
            "Epoch 1/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 2/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 3/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 4/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 5/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 6/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 7/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 8/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 9/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 10/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 11/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 12/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 13/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 14/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 15/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 16/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 17/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 18/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 19/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 20/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 21/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 22/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 23/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 24/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 25/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 26/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 27/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 28/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 29/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 30/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 31/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 32/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 33/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 34/80\n",
            "15/15 - 23s - loss: nan - val_loss: nan - 23s/epoch - 2s/step\n",
            "Epoch 35/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 36/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 37/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 38/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 39/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 40/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 41/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 42/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 43/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 44/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 45/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 46/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 47/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 48/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 49/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 50/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 51/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 52/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 53/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 54/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 55/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 56/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 57/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 58/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 59/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 60/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 61/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 62/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 63/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 64/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 65/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 66/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 67/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 68/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 69/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 70/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 71/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 72/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 73/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 74/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 75/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 76/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 77/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 78/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 79/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 80/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Metrics :  nan\n",
            "K Fold Step   4\n",
            "Epoch 1/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 2/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 3/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 4/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 5/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 6/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 7/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 8/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 9/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 10/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 11/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 12/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 13/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 14/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 15/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 16/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 17/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 18/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 19/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 20/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 21/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 22/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 23/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 24/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 25/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 26/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 27/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 28/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 29/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 30/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 31/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 32/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 33/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 34/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 35/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 36/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 37/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 38/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 39/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 40/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 41/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 42/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 43/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 44/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 45/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 46/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 47/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 48/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 49/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 50/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 51/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 52/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 53/80\n",
            "15/15 - 21s - loss: nan - val_loss: nan - 21s/epoch - 1s/step\n",
            "Epoch 54/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 55/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 56/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 57/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 58/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 59/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 60/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 61/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 62/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 63/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 64/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 65/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 66/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 67/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 68/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 69/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 70/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 71/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 72/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 73/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 74/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 75/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 76/80\n",
            "15/15 - 19s - loss: nan - val_loss: nan - 19s/epoch - 1s/step\n",
            "Epoch 77/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 78/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Epoch 79/80\n",
            "15/15 - 20s - loss: nan - val_loss: nan - 20s/epoch - 1s/step\n",
            "Epoch 80/80\n",
            "15/15 - 18s - loss: nan - val_loss: nan - 18s/epoch - 1s/step\n",
            "Metrics :  nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(os.getcwd()+\"/speaker_classifier.h5\", compile= False)\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Load the standard deviation and mean\n",
        "f = open(os.getcwd()+'/speaker_mean_std.pkl', 'rb')\n",
        "mean, std, poss_knnn = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "testing_data = np.load(os.getcwd()+\"/testing_features.npz\")\n",
        "X = testing_data['a']\n",
        "y = testing_data['b']\n",
        "ind = testing_data['c']\n",
        "unq_ind = np.unique(ind)\n",
        "\n",
        "X = X.astype(np.float16)\n",
        "\n",
        "X = X.transpose(0,2,1) # Put data in correct format: Num_samples x timesteps x features\n",
        "X = (X-mean)/ std\n",
        "\n",
        "# Predict\n",
        "ypred = model.predict(X)\n",
        "ypred = np.argmax(ypred, axis=-1)\n",
        "ypred = ypred.flatten()\n",
        "new_pred = []\n",
        "new_truth = []\n",
        "\n",
        "# Find unique ids and assign class based on majority vote from all the frames\n",
        "for i in range(len(unq_ind)) :\n",
        "    curr = unq_ind[i]\n",
        "    indices, = np.where(ind == curr)\n",
        "    t = y[indices]\n",
        "    t = t[0]\n",
        "\n",
        "    p = ypred[indices]\n",
        "    # p1 = ypred1[indices]\n",
        "    # p2 = ypred2[indices]\n",
        "    # p3 = ypred3[indices]\n",
        "    # p4 = ypred4[indices]\n",
        "\n",
        "    # all_model_pred = [get_best_candidate(x, t) for x in [p1, p2, p3, p4]]\n",
        "    # un, fr = np.unique(all_model_pred, return_counts=True)\n",
        "\n",
        "    un, fr = np.unique(p, return_counts=True)\n",
        "    new_pred.append(un[np.argmax(fr)])\n",
        "    print(\"Truth \", t, \" Pred \", un, fr)\n",
        "\n",
        "    new_truth.append(t)\n",
        "\n",
        "new_truth = np.array(new_truth)\n",
        "new_pred = np.array(new_pred)\n",
        "\n",
        "# Print classification report\n",
        "rep = classification_report(new_truth, new_pred, target_names=['speaker1', 'speaker2', 'speaker3'])\n",
        "print(rep)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rHqUfeJqKXN",
        "outputId": "74957e02-6cc5-4756-adfd-60d2fa773f3e"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 400, 76)]         0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 400, 32)           7328      \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 400, 32)           128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " activation (Activation)     (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_3 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_4 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_8 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_9 (Bat  (None, 400, 32)           128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_10 (Conv1D)          (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_10 (Ba  (None, 400, 32)           128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_11 (Conv1D)          (None, 400, 32)           3104      \n",
            "                                                                 \n",
            " batch_normalization_11 (Ba  (None, 400, 32)           128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 400, 32)           0         \n",
            "                                                                 \n",
            " conv1d_12 (Conv1D)          (None, 400, 16)           1552      \n",
            "                                                                 \n",
            " batch_normalization_12 (Ba  (None, 400, 16)           64        \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 400, 16)           0         \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 16)                0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 3)                 51        \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 3)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44675 (174.51 KB)\n",
            "Trainable params: 43875 (171.39 KB)\n",
            "Non-trainable params: 800 (3.12 KB)\n",
            "_________________________________________________________________\n",
            "None\n",
            "134/134 [==============================] - 11s 78ms/step\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [15]\n",
            "Truth  1  Pred  [0] [15]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [5]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [4]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [13]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [13]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [5]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [6]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [5]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [5]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [17]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  2  Pred  [0] [13]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [17]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  2  Pred  [0] [17]\n",
            "Truth  2  Pred  [0] [13]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [5]\n",
            "Truth  2  Pred  [0] [20]\n",
            "Truth  1  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [5]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [5]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [17]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [5]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [14]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [16]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [13]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [15]\n",
            "Truth  2  Pred  [0] [18]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  2  Pred  [0] [13]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [13]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [13]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [14]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [14]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [20]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [13]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [14]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [12]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [16]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [6]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [8]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [12]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  1  Pred  [0] [13]\n",
            "Truth  0  Pred  [0] [11]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [10]\n",
            "Truth  2  Pred  [0] [13]\n",
            "Truth  2  Pred  [0] [9]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  0  Pred  [0] [7]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  2  Pred  [0] [14]\n",
            "Truth  1  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [10]\n",
            "Truth  0  Pred  [0] [9]\n",
            "Truth  2  Pred  [0] [11]\n",
            "Truth  1  Pred  [0] [8]\n",
            "Truth  0  Pred  [0] [10]\n",
            "Truth  1  Pred  [0] [9]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    speaker1       0.35      1.00      0.52       158\n",
            "    speaker2       0.00      0.00      0.00       152\n",
            "    speaker3       0.00      0.00      0.00       140\n",
            "\n",
            "    accuracy                           0.35       450\n",
            "   macro avg       0.12      0.33      0.17       450\n",
            "weighted avg       0.12      0.35      0.18       450\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}